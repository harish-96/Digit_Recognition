<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Neural_Network.neuralnet &#8212; Digit_Recognition 1 documentation</title>
    
    <link rel="stylesheet" href="../../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     '1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="top" title="Digit_Recognition 1 documentation" href="../../index.html" />
    <link rel="up" title="Module code" href="../index.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body role="document">
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <h1>Source code for Neural_Network.neuralnet</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Neural Network</span>
<span class="sd">~~~~~~~~~~~~~~~~</span>
<span class="sd">A module to train a neural network with an arbitrary number of layers and</span>
<span class="sd">and arbitrary number of neurons per layer. Use as::</span>
<span class="sd">    from neuralnet import NN_hwr</span>
<span class="sd">    nn = NN_hwr([list containing number of neurons per layer])</span>
<span class="sd">    nn.train_nn(Input training data, label_data, Number of iterations,</span>
<span class="sd">                Number of examples per batch, learning_rate)</span>
<span class="sd">&quot;&quot;&quot;</span>


<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">scipy.io</span> <span class="k">as</span> <span class="nn">sio</span>


<div class="viewcode-block" id="sigmoid"><a class="viewcode-back" href="../../Neural_Network.html#Neural_Network.neuralnet.sigmoid">[docs]</a><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Evaluates the sigmoid function at the given input</span>

<span class="sd">    :param array-like z: Could be a number, list or Numpy array for</span>
<span class="sd">        which sigmoid is to be evaluated</span>

<span class="sd">    :return: numpy array&quot;&quot;&quot;</span>

    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="k">return</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span></div>


<div class="viewcode-block" id="sigmoid_derivative"><a class="viewcode-back" href="../../Neural_Network.html#Neural_Network.neuralnet.sigmoid_derivative">[docs]</a><span class="k">def</span> <span class="nf">sigmoid_derivative</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Evaluates the derivative of the sigmoid function at the given input</span>

<span class="sd">    :param array-like z: Could be a number, list or Numpy array for which</span>
<span class="sd">        sigmoid derivative is to be evaluated</span>

<span class="sd">    :return: Numpy array&quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">))</span></div>


<div class="viewcode-block" id="NN_hwr"><a class="viewcode-back" href="../../Neural_Network.html#Neural_Network.neuralnet.NN_hwr">[docs]</a><span class="k">class</span> <span class="nc">NN_hwr</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A template class for a generic neural network.</span>
<span class="sd">    Initialise with a list with each element being the number of neurons in</span>
<span class="sd">    that layer</span>
<span class="sd">    For the init function, the parameters are</span>

<span class="sd">    :param list num_neurons_list: Create a neural network number</span>
<span class="sd">        of neuron per layer given by the list</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_neurons_list</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Input must be a list of numbers</span>

<span class="sd">        :param list num_neurons_list: Create a neural network number of</span>
<span class="sd">            neuron per layer given by the list</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">num_neurons_list</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">type</span><span class="p">(</span><span class="mi">2</span><span class="p">)]:</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Expected integer type&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">num_neurons_list</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_neurons_list</span> <span class="o">=</span> <span class="n">num_neurons_list</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">num_neurons_list</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">num_neurons_list</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                        <span class="n">num_neurons_list</span><span class="p">[</span><span class="mi">1</span><span class="p">:])]</span>

<div class="viewcode-block" id="NN_hwr.forward_prop"><a class="viewcode-back" href="../../Neural_Network.html#Neural_Network.neuralnet.NN_hwr.forward_prop">[docs]</a>    <span class="k">def</span> <span class="nf">forward_prop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_train</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Computes the activations and weighted inputs of the neurons in</span>
<span class="sd">        the network for the given input data.</span>

<span class="sd">        :param ndarray x_train: The input for the first layer which needs</span>
<span class="sd">            to be forwards propogated</span>

<span class="sd">        :return: A tuple of lists containing activations and weighted inputs</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">activations</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">z</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">activations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">z</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">activations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

        <span class="k">return</span> <span class="n">activations</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">z</span></div>

<div class="viewcode-block" id="NN_hwr.back_prop"><a class="viewcode-back" href="../../Neural_Network.html#Neural_Network.neuralnet.NN_hwr.back_prop">[docs]</a>    <span class="k">def</span> <span class="nf">back_prop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_example</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Computes the partial derivatives of the cost function with respect to</span>
<span class="sd">        the weights and biases of the network</span>
<span class="sd">        training_example is a tuple with element fisrt element an np array and</span>
<span class="sd">        the second, an array of length 10 of zeros everywhere except at the</span>
<span class="sd">        image label where there is a 1</span>

<span class="sd">        :param tuple training_example: Tuple with first element as the input</span>
<span class="sd">            data and the second being its label</span>

<span class="sd">        :return: a tuple of numpy arrays containing the required derivatives&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">training_example</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Expected input of size 2&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">training_example</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">training_example</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Expected list with 1st element</span><span class="se">\</span>
<span class="s2">                                 being a 784 x 1 numpy array&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Expected a numpy array for</span><span class="se">\</span>
<span class="s2">                            first element of input&quot;</span><span class="p">)</span>

        <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">training_example</span>
        <span class="n">activations</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_prop</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>

        <span class="n">delta_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">[:])</span>
        <span class="n">delta_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[:])</span>

        <span class="n">delta_L</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cost_derivative</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                                       <span class="n">y_train</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigmoid_derivative</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">delta_b</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta_L</span>
        <span class="n">delta_w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">delta_L</span><span class="p">,</span> <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">delta_L</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="n">sd</span> <span class="o">=</span> <span class="n">sigmoid_derivative</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="o">-</span><span class="n">i</span><span class="p">])</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="o">-</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">transpose</span><span class="p">(),</span> <span class="n">delta</span><span class="p">)</span> <span class="o">*</span> <span class="n">sd</span>
            <span class="n">delta_b</span><span class="p">[</span><span class="o">-</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">ac</span> <span class="o">=</span> <span class="n">x_train</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ac</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
            <span class="n">delta_w</span><span class="p">[</span><span class="o">-</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">ac</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">delta_b</span><span class="p">,</span> <span class="n">delta_w</span><span class="p">)</span></div>

<div class="viewcode-block" id="NN_hwr.train_batch"><a class="viewcode-back" href="../../Neural_Network.html#Neural_Network.neuralnet.NN_hwr.train_batch">[docs]</a>    <span class="k">def</span> <span class="nf">train_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Trains the network with one subset of the training data.</span>
<span class="sd">        Input is the subset of training data for witch the network is</span>
<span class="sd">        to be trained. Learning rate governs the rate at which the</span>
<span class="sd">        Weights and biases change in the gradient descent algorithm</span>

<span class="sd">        :param ndarray batch: An array of training examples with each</span>
<span class="sd">            being a tuple containing the input data and its label.</span>

<span class="sd">        :param float learning_rate: The learning rate which determines</span>
<span class="sd">            the step size in gradient descent</span>

<span class="sd">        :return: None</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">delta_b_sum</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">]</span>
        <span class="n">delta_w_sum</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">training_example</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span>
            <span class="n">delta_b</span><span class="p">,</span> <span class="n">delta_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">back_prop</span><span class="p">(</span><span class="n">training_example</span><span class="p">)</span>
            <span class="n">delta_b_sum</span> <span class="o">+=</span> <span class="n">delta_b</span>
            <span class="n">delta_w_sum</span> <span class="o">+=</span> <span class="n">delta_w</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">delta_b_sum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">delta_w_sum</span></div>

<div class="viewcode-block" id="NN_hwr.train_nn"><a class="viewcode-back" href="../../Neural_Network.html#Neural_Network.neuralnet.NN_hwr.train_nn">[docs]</a>    <span class="k">def</span> <span class="nf">train_nn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Trains the neural network with the test data. n_epochs is the number</span>
<span class="sd">        sweeps over the whole data. batch_size is the number of training</span>
<span class="sd">        example per batch in the stochastic gradient descent. X_train and</span>
<span class="sd">        y_train are the images and labels in the training data. X_train must</span>
<span class="sd">        be a 2-D array with only one row and y_train is an array of length 10</span>
<span class="sd">        of zeros everywhere except at the image label (where there is a 1)</span>

<span class="sd">        :param ndarray X_train: Numpy array containing the input training data</span>
<span class="sd">        :param ndarray y_train: Numpy array containing the labels for training.</span>
<span class="sd">            Formatted as an array of arrays with 1 at the label position and 0</span>
<span class="sd">            everywhere else</span>

<span class="sd">        :param int n_epochs: The number of sweeps over the data-set in the</span>
<span class="sd">            Stochastic Gradient Descent</span>

<span class="sd">        :param int batch_size: Number of training examples per batch in the</span>
<span class="sd">            Stochastic Gradient Descent</span>

<span class="sd">        :param fload learning_rate: The learning rate which determines the</span>
<span class="sd">            step size in gradient descent</span>

<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">m</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
        <span class="n">train_data</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
            <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
            <span class="n">batches</span> <span class="o">=</span> <span class="p">[</span><span class="n">train_data</span><span class="p">[</span><span class="n">j</span><span class="p">:</span><span class="n">j</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">]</span>
                       <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)]</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train_batch</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;epoch no: </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cost_function</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
        <span class="n">sio</span><span class="o">.</span><span class="n">savemat</span><span class="p">(</span><span class="s1">&#39;../data/weights_biases&#39;</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;w&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span>
                    <span class="s1">&#39;b&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">})</span></div>

<div class="viewcode-block" id="NN_hwr.cost_function"><a class="viewcode-back" href="../../Neural_Network.html#Neural_Network.neuralnet.NN_hwr.cost_function">[docs]</a>    <span class="k">def</span> <span class="nf">cost_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Computes the quadratic cost function of the Neural Network</span>

<span class="sd">        :param ndarray X_train: Input data for training</span>
<span class="sd">        :param ndarray y_train: Labels corresponding to the inputs</span>

<span class="sd">        :return: Float value of the cost function of the Neural Network</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">J</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)):</span>
            <span class="n">J</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">forward_prop</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                               <span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">y_train</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">J</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span></div>

<div class="viewcode-block" id="NN_hwr.cost_derivative"><a class="viewcode-back" href="../../Neural_Network.html#Neural_Network.neuralnet.NN_hwr.cost_derivative">[docs]</a>    <span class="k">def</span> <span class="nf">cost_derivative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">activation</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Computes the derivative of the cost function given output activations and</span>
<span class="sd">        the labels</span>

<span class="sd">        :param ndarray activation: Activations of the neurons for a given input</span>
<span class="sd">        :param list y: The expected output for the input</span>

<span class="sd">        :return: Float value of the cost function of the Neural Network</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span></div>

<div class="viewcode-block" id="NN_hwr.accuracy"><a class="viewcode-back" href="../../Neural_Network.html#Neural_Network.neuralnet.NN_hwr.accuracy">[docs]</a>    <span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Computes the accuracy with which the Network, once trained classifies</span>
<span class="sd">        the digits.</span>

<span class="sd">        :param ndarray X_test: Test data in the same format as that for which</span>
<span class="sd">            the network was trained</span>

<span class="sd">        :param ndarray y_test: Labels for the test data. Same format as the</span>
<span class="sd">            labels for training data</span>

<span class="sd">        :return: Float value of the accuracy in percentage</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)):</span>
            <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_prop</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">])[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_test</span><span class="p">[</span><span class="n">i</span><span class="p">])[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]:</span>
                <span class="n">accuracy</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span> <span class="o">*</span> <span class="mf">100.0</span>
        <span class="k">return</span> <span class="n">accuracy</span></div></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  <li><a href="../index.html">Module code</a><ul>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2016, Harish Murali and Surya Mohan.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.4.8</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.9</a>
      
    </div>

    

    
  </body>
</html>